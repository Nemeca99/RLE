#!/usr/bin/env python3
"""
Bulletproof Single-Session Harness for Thermal-Optimization Coupling
Implements shared clock synchronization and atomic experiment protocol
"""

import subprocess
import time
import json
import os
import psutil
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Try to import nvidia_ml_py3, fallback if not available
try:
    import nvidia_ml_py3 as nvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("Warning: nvidia_ml_py3 not available, using fallback GPU monitoring")

class BulletproofSessionHarness:
    """Atomic experiment harness with shared clock synchronization"""
    
    def __init__(self, lab_dir="F:/RLE/lab", model_dir="L:/models/luna_trained_final"):
        self.lab_dir = lab_dir
        self.model_dir = model_dir
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = Path(self.lab_dir) / "sessions" / "bulletproof" / self.session_id
        self.session_dir.mkdir(parents=True, exist_ok=True)
        
        # Shared clock for synchronization
        self.t0 = None
        self.metadata = {}
        
    def get_shared_timestamp(self):
        """Get elapsed time since session start"""
        if self.t0 is None:
            self.t0 = time.time()
        return time.time() - self.t0
    
    def collect_ambient_metadata(self):
        """Collect session metadata with default values"""
        print("="*70)
        print("BULLETPROOF SESSION METADATA COLLECTION")
        print("="*70)
        
        # Use default ambient temperature (21°C as specified)
        ambient_temp = "21.0"
        
        # Get GPU baseline temperature
        if NVML_AVAILABLE:
            try:
                nvml.nvmlInit()
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                gpu_start_temp = gpu_temp
            except:
                gpu_start_temp = "unknown"
        else:
            gpu_start_temp = "unknown"
        
        # Use default session notes
        notes = "Bulletproof atomic experiment with shared clock"
        
        # Use default run type
        run_type = "gpu_finetune"
        
        self.metadata = {
            "session_id": self.session_id,
            "session_start": datetime.now().isoformat(),
            "ambient_temp_c": float(ambient_temp),
            "gpu_start_temp_c": gpu_start_temp,
            "run_type": run_type,
            "notes": notes,
            "harness_version": "bulletproof_v1.0",
            "shared_clock": True
        }
        
        print(f"Session ID: {self.session_id}")
        print(f"Ambient temp: {ambient_temp}°C")
        print(f"GPU start temp: {gpu_start_temp}°C")
        print(f"Run type: {run_type}")
        print(f"Notes: {notes}")
        
        return self.metadata
    
    def create_synchronized_training_script(self):
        """Create training script with shared clock synchronization"""
        training_script = f"""
#!/usr/bin/env python3
\"\"\"
Synchronized Training Script with Shared Clock
Generated by BulletproofSessionHarness
\"\"\"

import time
import json
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from datasets import Dataset
import numpy as np

class SynchronizedGradNormLogger:
    def __init__(self, log_file, session_start_time):
        self.log_file = log_file
        self.session_start_time = session_start_time
        self.logs = []
        self.step_counter = 0
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            logs = {{}}
        
        self.step_counter += 1
        
        # Use shared clock: elapsed time since session start
        elapsed_time = time.time() - self.session_start_time
        
        log_entry = {{
            "step": state.global_step,
            "step_counter": self.step_counter,
            "elapsed_time": elapsed_time,  # Shared clock
            "timestamp_shared": elapsed_time,
            "timestamp_iso": time.strftime("%Y-%m-%dT%H:%M:%S"),
            "loss": logs.get("loss"),
            "grad_norm": self._get_grad_norm(kwargs.get("model")),
            "learning_rate": logs.get("learning_rate"),
            "epoch": logs.get("epoch")
        }}
        
        self.logs.append(log_entry)
        
        # Save immediately for real-time access
        with open(self.log_file, 'w') as f:
            json.dump(self.logs, f, indent=2)
    
    def _get_grad_norm(self, model):
        if model is None:
            return None
        
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        return total_norm ** (1. / 2)

def main():
    # Initialize shared clock
    session_start_time = time.time()
    
    # Load model and tokenizer
    model_name = "distilgpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Create synthetic dataset for consistent workload
    def create_synthetic_dataset():
        texts = [
            "The quick brown fox jumps over the lazy dog. " * 10,
            "Machine learning is transforming the world of artificial intelligence. " * 10,
            "Thermal efficiency monitoring provides insights into system performance. " * 10,
            "Gradient norms indicate optimization stability during training. " * 10,
            "Real-time monitoring enables proactive system management. " * 10
        ] * 20  # Repeat for more data
        
        return Dataset.from_dict({{"text": texts}})
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=128,
            return_tensors=None
        )
    
    # Prepare dataset
    dataset = create_synthetic_dataset()
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # Training arguments for stable workload
    training_args = TrainingArguments(
        output_dir="./training_output",
        num_train_epochs=1,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=10,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=1,  # Log every step for fine-grained analysis
        save_steps=1000,
        eval_strategy="no",
        report_to=None,
        max_steps=120,  # 2 minutes at ~1 step/second
        learning_rate=5e-5,
        lr_scheduler_type="linear"
    )
    
    # Initialize logger
    log_file = "{self.session_dir}/synchronized_training_log.json"
    grad_logger = SynchronizedGradNormLogger(log_file, session_start_time)
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        callbacks=[grad_logger]
    )
    
    print("Starting synchronized training...")
    print(f"Session start time: {{session_start_time}}")
    print(f"Log file: {{log_file}}")
    
    # Train
    trainer.train()
    
    print("Training complete!")
    print(f"Total steps logged: {{len(grad_logger.logs)}}")
    print(f"Step counter range: 0-{{grad_logger.step_counter}}")

if __name__ == "__main__":
    main()
"""
        
        script_path = self.session_dir / "synchronized_training.py"
        with open(script_path, 'w') as f:
            f.write(training_script)
        
        return script_path
    
    def create_synchronized_rle_script(self):
        """Create RLE monitoring script with shared clock"""
        rle_script = f"""
#!/usr/bin/env python3
\"\"\"
Synchronized RLE Monitoring with Shared Clock
Generated by BulletproofSessionHarness
\"\"\"

import time
import csv
import json
import psutil
import nvidia_ml_py3 as nvml
from pathlib import Path
import numpy as np

class SynchronizedRLEMonitor:
    def __init__(self, session_dir, session_start_time, duration=120):
        self.session_dir = session_dir
        self.session_start_time = session_start_time
        self.duration = duration
        
        # Initialize NVML
        if NVML_AVAILABLE:
            try:
                nvml.nvmlInit()
                self.gpu_handle = nvml.nvmlDeviceGetHandleByIndex(0)
                self.gpu_available = True
            except:
                self.gpu_available = False
        else:
            self.gpu_available = False
        
        # CSV setup
        self.csv_file = session_dir / "synchronized_rle_log.csv"
        self.fieldnames = [
            "elapsed_time", "timestamp_shared", "device", "rle_smoothed", 
            "rle_raw", "temp_c", "power_w", "util_pct", "a_load", 
            "t_sustain_s", "rolling_peak", "collapse", "alerts"
        ]
        
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
            writer.writeheader()
        
        # RLE state
        self.rolling_peaks = {{"cpu": 0.0, "gpu": 0.0}}
        self.collapse_counters = {{"cpu": 0, "gpu": 0}}
        self.rle_history = {{"cpu": [], "gpu": []}}
    
    def get_shared_timestamp(self):
        return time.time() - self.session_start_time
    
    def compute_rle(self, util_pct, temp_c, power_w, device):
        # Simplified RLE computation
        if util_pct < 1:
            return 0.0
        
        # Stability factor (higher temp = lower stability)
        stability = max(0.1, 1.0 - (temp_c - 30) / 50)
        
        # Load factor
        a_load = min(1.0, power_w / 100)  # Normalize to 100W
        
        # Sustain time (simplified)
        t_sustain = max(1.0, 60 - (temp_c - 30) * 2)
        
        # RLE computation
        rle = (util_pct / 100) * stability / (a_load * (1 + 1/t_sustain))
        
        return rle
    
    def detect_collapse(self, rle, device):
        # Update rolling peak
        if rle > self.rolling_peaks[device]:
            self.rolling_peaks[device] = rle
        else:
            # Decay rolling peak
            self.rolling_peaks[device] *= 0.998
        
        # Collapse detection: RLE drops below 65% of rolling peak
        threshold = self.rolling_peaks[device] * 0.65
        collapse = 1 if rle < threshold else 0
        
        if collapse:
            self.collapse_counters[device] += 1
        
        return collapse
    
    def get_cpu_metrics(self):
        try:
            util_pct = psutil.cpu_percent(interval=0.1)
            temp_c = 50.0  # Placeholder - would need proper sensor
            power_w = 25.0  # Placeholder
            return util_pct, temp_c, power_w
        except:
            return 0.0, 50.0, 25.0
    
    def get_gpu_metrics(self):
        if not self.gpu_available or not NVML_AVAILABLE:
            return 0.0, 50.0, 0.0
        
        try:
            util = nvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)
            util_pct = util.gpu
            temp_c = nvml.nvmlDeviceGetTemperature(self.gpu_handle, nvml.NVML_TEMPERATURE_GPU)
            power_w = nvml.nvmlDeviceGetPowerUsage(self.gpu_handle) / 1000
            return util_pct, temp_c, power_w
        except:
            return 0.0, 50.0, 0.0
    
    def log_sample(self):
        elapsed_time = self.get_shared_timestamp()
        
        # CPU metrics
        cpu_util, cpu_temp, cpu_power = self.get_cpu_metrics()
        cpu_rle = self.compute_rle(cpu_util, cpu_temp, cpu_power, "cpu")
        cpu_collapse = self.detect_collapse(cpu_rle, "cpu")
        
        # GPU metrics
        gpu_util, gpu_temp, gpu_power = self.get_gpu_metrics()
        gpu_rle = self.compute_rle(gpu_util, gpu_temp, gpu_power, "gpu")
        gpu_collapse = self.detect_collapse(gpu_rle, "gpu")
        
        # Log both devices
        samples = [
            {{
                "elapsed_time": elapsed_time,
                "timestamp_shared": elapsed_time,
                "device": "cpu",
                "rle_smoothed": cpu_rle,
                "rle_raw": cpu_rle,
                "temp_c": cpu_temp,
                "power_w": cpu_power,
                "util_pct": cpu_util,
                "a_load": min(1.0, cpu_power / 100),
                "t_sustain_s": max(1.0, 60 - (cpu_temp - 30) * 2),
                "rolling_peak": self.rolling_peaks["cpu"],
                "collapse": cpu_collapse,
                "alerts": ""
            }},
            {{
                "elapsed_time": elapsed_time,
                "timestamp_shared": elapsed_time,
                "device": "gpu",
                "rle_smoothed": gpu_rle,
                "rle_raw": gpu_rle,
                "temp_c": gpu_temp,
                "power_w": gpu_power,
                "util_pct": gpu_util,
                "a_load": min(1.0, gpu_power / 100),
                "t_sustain_s": max(1.0, 60 - (gpu_temp - 30) * 2),
                "rolling_peak": self.rolling_peaks["gpu"],
                "collapse": gpu_collapse,
                "alerts": ""
            }}
        ]
        
        # Write to CSV
        with open(self.csv_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
            writer.writerows(samples)
        
        return samples
    
    def run(self):
        print("Starting synchronized RLE monitoring...")
        print(f"Session start time: {{self.session_start_time}}")
        print(f"Duration: {{self.duration}} seconds")
        print(f"Log file: {{self.csv_file}}")
        
        start_time = time.time()
        sample_count = 0
        
        while time.time() - start_time < self.duration:
            samples = self.log_sample()
            sample_count += 2  # CPU + GPU
            
            if sample_count % 20 == 0:  # Every 20 samples
                elapsed = time.time() - start_time
                print(f"[{{elapsed:.1f}}s] Samples: {{sample_count}}, "
                      f"CPU RLE: {{samples[0]['rle_smoothed']:.3f}}, "
                      f"GPU RLE: {{samples[1]['rle_smoothed']:.3f}}")
            
            time.sleep(1.0)  # 1 Hz sampling
        
        print("RLE monitoring complete!")
        print(f"Total samples: {{sample_count}}")

def main():
    session_dir = Path("{self.session_dir}")
    session_start_time = time.time()
    
    monitor = SynchronizedRLEMonitor(session_dir, session_start_time, duration=120)
    monitor.run()

if __name__ == "__main__":
    main()
"""
        
        script_path = self.session_dir / "synchronized_rle_monitor.py"
        with open(script_path, 'w') as f:
            f.write(rle_script)
        
        return script_path
    
    def run_atomic_experiment(self):
        """Run complete atomic experiment with shared clock"""
        print("="*70)
        print("BULLETPROOF ATOMIC EXPERIMENT")
        print("="*70)
        
        # Collect metadata
        self.collect_ambient_metadata()
        
        # Create synchronized scripts
        training_script = self.create_synchronized_training_script()
        rle_script = self.create_synchronized_rle_script()
        
        # Initialize shared clock
        self.t0 = time.time()
        self.metadata["session_start_timestamp"] = self.t0
        
        print(f"\\nStarting atomic experiment...")
        print(f"Session ID: {{self.session_id}}")
        print(f"Session directory: {{self.session_dir}}")
        print(f"Shared clock start: {{self.t0}}")
        
        # Start RLE monitoring in background
        print("\\n[1/3] Starting RLE monitoring...")
        rle_process = subprocess.Popen([
            f"{self.lab_dir}/../venv/Scripts/python.exe",
            str(rle_script)
        ], cwd=self.lab_dir)
        
        # Wait for RLE to stabilize
        print("[2/3] Stabilizing baseline (2s)...")
        time.sleep(2)
        
        # Start training
        print("[3/3] Starting synchronized training...")
        training_process = subprocess.run([
            f"{self.lab_dir}/../venv/Scripts/python.exe",
            str(training_script)
        ], cwd=self.model_dir)
        
        # Wait for RLE to finish
        print("\\nWaiting for RLE monitoring to complete...")
        rle_process.wait()
        
        # Collect final metadata
        if NVML_AVAILABLE:
            try:
                nvml.nvmlInit()
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_end_temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
            except:
                gpu_end_temp = "unknown"
        else:
            gpu_end_temp = "unknown"
        
        self.metadata.update({
            "session_end": datetime.now().isoformat(),
            "gpu_end_temp_c": gpu_end_temp,
            "training_exit_code": training_process.returncode,
            "rle_exit_code": rle_process.returncode,
            "session_duration_seconds": time.time() - self.t0
        })
        
        # Save metadata
        metadata_file = self.session_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
        
        print(f"\\n✅ Atomic experiment complete!")
        print(f"Session directory: {{self.session_dir}}")
        print(f"Metadata: {{metadata_file}}")
        print(f"RLE log: {{self.session_dir / 'synchronized_rle_log.csv'}}")
        print(f"Training log: {{self.session_dir / 'synchronized_training_log.json'}}")
        
        return self.session_dir
    
    def analyze_session(self):
        """Analyze the completed session"""
        print("\\n" + "="*70)
        print("SESSION ANALYSIS")
        print("="*70)
        
        # Load data
        rle_file = self.session_dir / "synchronized_rle_log.csv"
        training_file = self.session_dir / "synchronized_training_log.json"
        
        if not rle_file.exists() or not training_file.exists():
            print("❌ Session files not found!")
            return None
        
        # Load RLE data
        rle_df = pd.read_csv(rle_file)
        
        # Load training data
        with open(training_file, 'r') as f:
            training_logs = json.load(f)
        
        training_df = pd.DataFrame(training_logs)
        
        # Align data using shared timestamps
        merged_data = pd.merge_asof(
            training_df.sort_values('timestamp_shared'),
            rle_df[rle_df['device'] == 'gpu'].sort_values('timestamp_shared'),
            on='timestamp_shared',
            direction='nearest',
            tolerance=1.0  # 1 second tolerance
        )
        
        if len(merged_data) == 0:
            print("❌ No aligned data found!")
            return None
        
        # Calculate correlations
        correlations = {
            'gpu_grad_rle': merged_data['grad_norm'].corr(merged_data['rle_smoothed']),
            'gpu_temp_grad': merged_data['temp_c'].corr(merged_data['grad_norm']),
            'gpu_loss_rle': merged_data['loss'].corr(merged_data['rle_smoothed'])
        }
        
        print(f"Aligned samples: {{len(merged_data)}}")
        print(f"GPU grad_norm ↔ RLE: {{correlations['gpu_grad_rle']:.3f}}")
        print(f"GPU temp ↔ grad_norm: {{correlations['gpu_temp_grad']:.3f}}")
        print(f"GPU loss ↔ RLE: {{correlations['gpu_loss_rle']:.3f}}")
        
        # Lag analysis
        print("\\nLag Analysis:")
        for lag in range(-3, 4):
            if lag < 0:
                corr = merged_data['grad_norm'].shift(lag).corr(merged_data['rle_smoothed'])
            elif lag > 0:
                corr = merged_data['grad_norm'].corr(merged_data['rle_smoothed'].shift(lag))
            else:
                corr = merged_data['grad_norm'].corr(merged_data['rle_smoothed'])
            
            print(f"  Lag {{lag:+2d}}s: {{corr:.3f}}")
        
        return correlations

def main():
    """Run bulletproof session harness"""
    harness = BulletproofSessionHarness()
    
    # Run atomic experiment
    session_dir = harness.run_atomic_experiment()
    
    # Analyze results
    correlations = harness.analyze_session()
    
    print("\\n" + "="*70)
    print("BULLETPROOF SESSION COMPLETE")
    print("="*70)
    print(f"Session ID: {{harness.session_id}}")
    print(f"Session directory: {{session_dir}}")
    
    if correlations:
        print("\\nKey Correlations:")
        for name, value in correlations.items():
            print(f"  {{name}}: {{value:.3f}}")

if __name__ == "__main__":
    main()
